{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ikjunerd/Ikjunerd/blob/main/robomimic_get_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Robomimic Get Started Tutorial\n",
        "\n",
        "This notebook implements a simple training loop without the extensive features offered in robomimic such as logging and hyperparameter sweeping. Please refer to the [repository](https://github.com/ARISE-Initiative/robomimic) and the [documentation](https://robomimic.github.io/docs/introduction/overview.html) for the full set of features and the rest of the pipeline.\n",
        "\n",
        "This notebook includes the following tutorials:\n",
        "\n",
        "1. Set up robomimic development environment\n",
        "2. Downloading task-specific dataset\n",
        "3. Create a naive behavior cloning policy\n",
        "4. Setup a simple training loop\n",
        "5. Run policy training\n",
        "6. Visualize the trained policy"
      ],
      "metadata": {
        "id": "DrbsbN3TtXmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###0. Use GPU to accelerate training\n",
        "\n",
        "To use GPU runtime, click runtime on the top navigation part -> change runtime type -> select GPU as your accelerator"
      ],
      "metadata": {
        "id": "qxIvWhDJGY52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Set up development environment\n",
        "\n",
        "The main dependencies of robomimic are\n",
        "- torch\n",
        "- numpy\n",
        "- h5py\n",
        "- robosuite\n",
        "- mujoco\n",
        "- tensorbordX\n",
        "- egl_probe\n",
        "- matplotlib\n",
        "\n",
        "\n",
        "The full list is included in the requirements.txt file in the repo."
      ],
      "metadata": {
        "id": "UbgiD1mkuGWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T1aZtB-M2DhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# First, we need to decide where to host the runtime storage\n",
        "USE_GDRIVE_STORAGE = False\n",
        "\n",
        "if not USE_GDRIVE_STORAGE:\n",
        "    # Option 1: use the colab runtime storage. All trained model and downloaded\n",
        "    # will disappear after you disconnect from the runtime.\n",
        "    WS_DIR = \"/content/\"\n",
        "else:\n",
        "    # Option 2: use your google drive as the runtime storage. You need to grant\n",
        "    # permission for the colab runtime to access your google drive. You also\n",
        "    # need to decide on a workspace for robomimic\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    WS_DIR = \"PATH-TO-YOUR-WORKSPACE\" # this should be the absolute path, e.g., \"/content/drive/MyDrive/my-ws/\"\n",
        "    assert os.path.exists(WS_DIR)\n",
        "\n",
        "%cd $WS_DIR"
      ],
      "metadata": {
        "id": "9Zw4cH9u4ZDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repo and install the basic requirements\n",
        "!git clone https://github.com/ARISE-Initiative/robomimic\n",
        "!pip install -e robomimic/\n",
        "\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('./robomimic/')"
      ],
      "metadata": {
        "id": "zvEgLOHNuFm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WARNING**: To exactly reproduce the setup from our study paper, mujoco and robosuite should be installed from source following the [instruction](https://robomimic.github.io/docs/introduction/installation.html#install-simulators) However, if you just want to train on our dataset, you could proceed with the\n",
        "following default setup."
      ],
      "metadata": {
        "id": "oT6VyAEUdZqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install mujoco and robosuite\n",
        "import os\n",
        "\n",
        "# install all system dependencies for mujoco-py\n",
        "!sudo apt install curl git libgl1-mesa-dev libgl1-mesa-glx libglew-dev \\\n",
        "         libosmesa6-dev software-properties-common net-tools unzip vim \\\n",
        "         virtualenv wget xpra xserver-xorg-dev libglfw3-dev patchelf\n",
        "\n",
        "#install mujoco-py\n",
        "!pip install mujoco\n",
        "\n",
        "#install robosuite\n",
        "!pip install robosuite"
      ],
      "metadata": {
        "id": "iNFm_2JOv2xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) test robomimic installation by running a dummy training loop\n",
        "!python robomimic/examples/train_bc_rnn.py --debug"
      ],
      "metadata": {
        "id": "-DBq5a2V01uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Download demonstration dataset for a task\n",
        "\n",
        "For robomimic tasks, we organize the demonstration datasets by\n",
        "- task name (e.g., lift)\n",
        "- data source (ph - proficient human, mh - multi human, mg - machine-generated)\n",
        "- observation type (low_dim or image)\n",
        "\n",
        "For more details of the dataset structure, visit [robomimic documentation](https://robomimic.github.io/docs/datasets/robomimic_v0.1.html) and the [dataset tutorial](https://github.com/ARISE-Initiative/robomimic/blob/master/examples/notebooks/datasets.ipynb)\n",
        "\n",
        "\n",
        "Here we demonstrate downloading the proficient human (`ph`) dataset with low-dimensional (`low_dim`) observation for the `lift` task.\n",
        "\n"
      ],
      "metadata": {
        "id": "xi6QiLK410TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "import robomimic\n",
        "import robomimic.utils.file_utils as FileUtils\n",
        "\n",
        "# the dataset registry can be found at robomimic/__init__.py\n",
        "from robomimic import DATASET_REGISTRY\n",
        "\n",
        "# set download folder and make it\n",
        "download_folder = WS_DIR + \"/robomimic_data/\"\n",
        "os.makedirs(download_folder, exist_ok=True)\n",
        "\n",
        "# download the dataset\n",
        "task = \"lift\"\n",
        "dataset_type = \"ph\"\n",
        "hdf5_type = \"low_dim\"\n",
        "FileUtils.download_url(\n",
        "    url=DATASET_REGISTRY[task][dataset_type][hdf5_type][\"url\"],\n",
        "    download_dir=download_folder,\n",
        ")\n",
        "\n",
        "# enforce that the dataset exists\n",
        "dataset_path = os.path.join(download_folder, \"low_dim_v141.hdf5\")\n",
        "assert os.path.exists(dataset_path)"
      ],
      "metadata": {
        "id": "Ptx48ZOK2GpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we can print out the data metadata\n",
        "!python robomimic/robomimic/scripts/get_dataset_info.py --dataset $dataset_path"
      ],
      "metadata": {
        "id": "s3uh3-sc1XQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Build a simple behavior cloning model\n",
        "\n",
        "Follows the default hyperparameter in `robomimic/config/bc_config.py`."
      ],
      "metadata": {
        "id": "ovLnbr8M4RzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import all utility functions\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import robomimic\n",
        "import robomimic.utils.obs_utils as ObsUtils\n",
        "import robomimic.utils.torch_utils as TorchUtils\n",
        "import robomimic.utils.test_utils as TestUtils\n",
        "import robomimic.utils.file_utils as FileUtils\n",
        "import robomimic.utils.train_utils as TrainUtils\n",
        "from robomimic.utils.dataset import SequenceDataset\n",
        "\n",
        "from robomimic.config import config_factory\n",
        "from robomimic.algo import algo_factory"
      ],
      "metadata": {
        "id": "VWIpC-Ob5VQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_example_model(dataset_path, device):\n",
        "    \"\"\"\n",
        "    Use a default config to construct a BC model.\n",
        "    \"\"\"\n",
        "\n",
        "    # default BC config\n",
        "    config = config_factory(algo_name=\"bc\")\n",
        "\n",
        "    # read config to set up metadata for observation modalities (e.g. detecting rgb observations)\n",
        "    ObsUtils.initialize_obs_utils_with_config(config)\n",
        "\n",
        "    # read dataset to get some metadata for constructing model\n",
        "    # all_obs_keys determines what observations we will feed to the policy\n",
        "    shape_meta = FileUtils.get_shape_metadata_from_dataset(\n",
        "        dataset_path=dataset_path,\n",
        "        all_obs_keys=sorted((\n",
        "            \"robot0_eef_pos\",  # robot end effector position\n",
        "            \"robot0_eef_quat\",   # robot end effector rotation (in quaternion)\n",
        "            \"robot0_gripper_qpos\",   # parallel gripper joint position\n",
        "            \"object\",  # object information\n",
        "        )),\n",
        "    )\n",
        "\n",
        "    # make BC model\n",
        "    model = algo_factory(\n",
        "        algo_name=config.algo_name,\n",
        "        config=config,\n",
        "        obs_key_shapes=shape_meta[\"all_shapes\"],\n",
        "        ac_dim=shape_meta[\"ac_dim\"],\n",
        "        device=device,\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "4fOznEH44g2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = TorchUtils.get_torch_device(try_to_use_cuda=True)\n",
        "model = get_example_model(dataset_path, device=device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "R_3knZqd4pGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Build a simple training loop\n",
        "\n",
        "Here we build a simple data loader pipeline and a training loop. Note that this code snippet is only instructional and is a stripped-down version of robomimic's main training loop (`robomimic/scripts/train.py`)."
      ],
      "metadata": {
        "id": "xxa1d9Te3CUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "WARNING: This code snippet is only for instructive purposes, and is missing several useful\n",
        "         components used during training such as logging and rollout evaluation.\n",
        "\"\"\"\n",
        "def get_data_loader(dataset_path):\n",
        "    \"\"\"\n",
        "    Get a data loader to sample batches of data.\n",
        "    Args:\n",
        "        dataset_path (str): path to the dataset hdf5\n",
        "    \"\"\"\n",
        "    dataset = SequenceDataset(\n",
        "        hdf5_path=dataset_path,\n",
        "        obs_keys=(                      # observations we want to appear in batches\n",
        "            \"robot0_eef_pos\",\n",
        "            \"robot0_eef_quat\",\n",
        "            \"robot0_gripper_qpos\",\n",
        "            \"object\",\n",
        "        ),\n",
        "        dataset_keys=(                  # can optionally specify more keys here if they should appear in batches\n",
        "            \"actions\",\n",
        "            \"rewards\",\n",
        "            \"dones\",\n",
        "        ),\n",
        "        load_next_obs=True,\n",
        "        frame_stack=1,\n",
        "        seq_length=10,                  # length-10 temporal sequences\n",
        "        pad_frame_stack=True,\n",
        "        pad_seq_length=True,            # pad last obs per trajectory to ensure all sequences are sampled\n",
        "        get_pad_mask=False,\n",
        "        goal_mode=None,\n",
        "        hdf5_cache_mode=\"all\",          # cache dataset in memory to avoid repeated file i/o\n",
        "        hdf5_use_swmr=True,\n",
        "        hdf5_normalize_obs=False,\n",
        "        filter_by_attribute=None,       # can optionally provide a filter key here\n",
        "    )\n",
        "    print(\"\\n============= Created Dataset =============\")\n",
        "    print(dataset)\n",
        "    print(\"\")\n",
        "\n",
        "    data_loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        sampler=None,       # no custom sampling logic (uniform sampling)\n",
        "        batch_size=100,     # batches of size 100\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        drop_last=True      # don't provide last batch in dataset pass if it's less than 100 in size\n",
        "    )\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "def run_train_loop(model, data_loader, num_epochs=50, gradient_steps_per_epoch=100):\n",
        "    \"\"\"\n",
        "    Note: this is a stripped down version of @TrainUtils.run_epoch and the train loop\n",
        "    in the train function in train.py. Logging and evaluation rollouts were removed.\n",
        "    Args:\n",
        "        model (Algo instance): instance of Algo class to use for training\n",
        "        data_loader (torch.utils.data.DataLoader instance): torch DataLoader for\n",
        "            sampling batches\n",
        "    \"\"\"\n",
        "    # ensure model is in train mode\n",
        "    model.set_train()\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1): # epoch numbers start at 1\n",
        "\n",
        "        # iterator for data_loader - it yields batches\n",
        "        data_loader_iter = iter(data_loader)\n",
        "\n",
        "        # record losses\n",
        "        losses = []\n",
        "\n",
        "        for _ in range(gradient_steps_per_epoch):\n",
        "\n",
        "            # load next batch from data loader\n",
        "            try:\n",
        "                batch = next(data_loader_iter)\n",
        "            except StopIteration:\n",
        "                # data loader ran out of batches - reset and yield first batch\n",
        "                data_loader_iter = iter(data_loader)\n",
        "                batch = next(data_loader_iter)\n",
        "\n",
        "            # process batch for training\n",
        "            input_batch = model.process_batch_for_training(batch)\n",
        "\n",
        "            # forward and backward pass\n",
        "            info = model.train_on_batch(batch=input_batch, epoch=epoch, validate=False)\n",
        "\n",
        "            # record loss\n",
        "            step_log = model.log_info(info)\n",
        "            losses.append(step_log[\"Loss\"])\n",
        "\n",
        "        # do anything model needs to after finishing epoch\n",
        "        model.on_epoch_end(epoch)\n",
        "\n",
        "        print(\"Train Epoch {}: Loss {}\".format(epoch, np.mean(losses)))\n"
      ],
      "metadata": {
        "id": "xkxeqO3B37YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Run policy training\n",
        "\n",
        "Using the model and the training loop defined above. Note that this simple training loop does not save checkpoint. For model checkpointing, take a look at the full-feature [training loop](https://github.com/ARISE-Initiative/robomimic/blob/master/robomimic/scripts/train.py#L290) and the [documentation](https://robomimic.github.io/docs/tutorials/viewing_results.html)"
      ],
      "metadata": {
        "id": "HUeL98Qr6lfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get dataset loader\n",
        "data_loader = get_data_loader(dataset_path=dataset_path)\n",
        "\n",
        "# run training loop\n",
        "run_train_loop(model=model, data_loader=data_loader, num_epochs=50, gradient_steps_per_epoch=100)"
      ],
      "metadata": {
        "id": "_aZq79x74J1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Evaluate and visualize trained policy\n",
        "\n",
        "Here we execute the trained policy `model` in a simulated environment and play the rollout video."
      ],
      "metadata": {
        "id": "4GozrJNd7-s8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create simulation environment\n",
        "\n",
        "import robomimic.utils.env_utils as EnvUtils\n",
        "\n",
        "env_meta = FileUtils.get_env_metadata_from_dataset(dataset_path)\n",
        "\n",
        "env = EnvUtils.create_env_from_metadata(\n",
        "    env_meta=env_meta,\n",
        "    env_name=env_meta[\"env_name\"],\n",
        "    render=False,\n",
        "    render_offscreen=True,\n",
        "    use_image_obs=False,\n",
        ")"
      ],
      "metadata": {
        "id": "x9Oe1qveJxE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from robomimic.algo import RolloutPolicy\n",
        "from robomimic.utils.train_utils import run_rollout\n",
        "import imageio\n",
        "\n",
        "# create a thin wrapper around the model to interact with the environment\n",
        "policy = RolloutPolicy(model)\n",
        "\n",
        "# create a video writer\n",
        "video_path = \"rollout.mp4\"\n",
        "video_writer = imageio.get_writer(video_path, fps=20)\n",
        "\n",
        "# run rollout\n",
        "rollout_log = run_rollout(\n",
        "    policy=policy,\n",
        "    env=env,\n",
        "    horizon=200,\n",
        "    video_writer=video_writer,\n",
        "    render=False\n",
        ")\n",
        "\n",
        "video_writer.close()\n",
        "# print rollout results\n",
        "print(rollout_log)"
      ],
      "metadata": {
        "id": "yboDeTXi7Qxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize rollout video\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open(video_path, \"rb\").read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "hm35ompoQ3V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What's next?\n",
        "\n",
        "Now that you understand the basic components of robomimic, it's time to delve deeper into each component by reading up the [documentation site](https://robomimic.github.io/docs/introduction/overview.html). Robomimic offers a rich set of utilities for model building, training loop management, model checkpointing, visualization, hyper-parameter sweeping, and logging. You can get to more about each component by following the provided the examples and tutorials.\n",
        "\n"
      ],
      "metadata": {
        "id": "zAVQQTf6SL7T"
      }
    }
  ]
}